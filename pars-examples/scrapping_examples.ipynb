{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Решение практических задач по парсингу данных"
      ],
      "metadata": {
        "id": "ekiJtkFz0TNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BeautifulSoup4"
      ],
      "metadata": {
        "id": "1KIWNdc24mgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Устанавливаем и импортируем необходимые библиотеки"
      ],
      "metadata": {
        "id": "VDOe4Nr24eVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем библиотеки\n",
        "! pip install fake-useragent\n",
        "import re\n",
        "import csv\n",
        "import time\n",
        "import requests\n",
        "\n",
        "from fake_useragent import UserAgent\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import urllib.parse\n",
        "from urllib.parse import urljoin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBMV2p8c4dfv",
        "outputId": "58a7240d-11cf-4aa4-84cc-b08d78836050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fake-useragent\n",
            "  Downloading fake_useragent-1.1.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fake-useragent\n",
            "Successfully installed fake-useragent-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Объявляем необходимые функции"
      ],
      "metadata": {
        "id": "WjV4nke76lRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# получение хедерса\n",
        "def get_headers(ajax=False) -> tuple:\n",
        "    '''\n",
        "    Получаем headers для requests\n",
        "\n",
        "    :param values: - браузеры для ресерча\n",
        "\n",
        "    Создаем случайного фейк-агента и возвращаем\n",
        "    '''\n",
        "    ua = UserAgent()\n",
        "\n",
        "    headers = {\n",
        "        'user-agent': ua.random,\n",
        "        'x-requested-with': 'XMLHttpRequest' if ajax else None\n",
        "    }\n",
        "\n",
        "    return headers\n",
        "\n",
        "# функция для получения ингредиентов супа\n",
        "def get_response(urln: str, pars='lxml',ajax=False ,enc='utf-8'):\n",
        "    '''\n",
        "    Получаем ингредиенты супа\n",
        "\n",
        "    :param urln: url который добавляем в суп\n",
        "    :param pars: какой парсинг будем использовать по умолчанию lxml\n",
        "    :param enc: кодировка для энкодинга\n",
        "\n",
        "    Получаем реквест по url энкодим по utf-8 по умолчанию\n",
        "    Получаем ингредиенты и возвращаем\n",
        "    '''\n",
        "\n",
        "    response = requests.get(url=urln, headers=get_headers(ajax))\n",
        "    response.encoding = 'utf-8'\n",
        "\n",
        "    soup = BeautifulSoup(response.text, pars)\n",
        "    return soup\n",
        "\n",
        "\n",
        "# Функция для извлечения чисел из строки\n",
        "def extract_numbers_from_string(string):\n",
        "    pattern = r'\\d+'  # Регулярное выражение для поиска цифр\n",
        "    numbers = re.findall(pattern, string)  # Находим все числа в строке\n",
        "    return numbers\n",
        "\n",
        "\n",
        "# параметры для ссылки на обменнике крипты\n",
        "def param_search(name: str, coin: str, count: int, revers=False) -> tuple:\n",
        "    data = {\n",
        "        'GiveName': name,\n",
        "        'GetName': coin,\n",
        "        'Sum': count,\n",
        "        'Direction': revers\n",
        "    }\n",
        "    return data\n",
        "\n",
        "\n",
        "# функция для округления float значений до 3-х знаков, даже если там 345.0\n",
        "def round_float(value: float) -> float:\n",
        "    '''\n",
        "    Функия для обрезания дополнительных нулевоезначениов для float\n",
        "\n",
        "    :param value: float значение\n",
        "\n",
        "    Создаемая функия обрезает дополнительные нулевоезначения для float\n",
        "    '''\n",
        "    if value == 0.0:\n",
        "        return 0.0\n",
        "\n",
        "    if value % 1 == 0.0:\n",
        "        return int(value)\n",
        "\n",
        "    if value % 1 == 0.5:\n",
        "        return int(value) + 1\n",
        "\n",
        "    if value % 1 == 0.25:\n",
        "        return int(value) + 0.5\n",
        "\n",
        "    if value % 1 == 0.75:\n",
        "        return int(value) + 0.5\n",
        "\n",
        "    if value % 1 == 0.1:\n",
        "        return int(value) + 0.1\n",
        "    return round(value, 3)\n"
      ],
      "metadata": {
        "id": "Y9LN_unk6s1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Решение задач"
      ],
      "metadata": {
        "id": "PVY2LO6V-fb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "DlL1IKJ5HR-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 1"
      ],
      "metadata": {
        "id": "EpslYORR0ceg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Открываем [сайт](http://parsinger.ru/html/index3_page_4.html)\n",
        "+ Проходимся по всем страницам в категории мыши (всего  4 страницы)\n",
        "+ На каждой странице посещаем каждую карточку с товаром (всего 32 товаров)\n",
        "+ В каждой карточке извлекаем при помощи bs4 артикул <p class=\"article\"> Артикул: 80244813 </p>\n",
        "+ Складываем(плюсуем) все собранные значения\n",
        "+ Выводим получившийся результат"
      ],
      "metadata": {
        "id": "wjq_rAy78j1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkWx0SZa5hbz"
      },
      "outputs": [],
      "source": [
        "for link in soup.find(class_='pagen').find_all('a'):\n",
        "    url_link = father_url + link['href']\n",
        "    resp = requests.get(url=url_link, headers=headers)\n",
        "    resp.encoding = 'utf-8'\n",
        "\n",
        "    soup = BeautifulSoup(resp.text, 'lxml')\n",
        "\n",
        "    named.append([name.text for name in soup.find(class_='item_card').find_all('a', class_='name_item')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyUUC1PO5hb0",
        "outputId": "49f9a578-2b38-4280-8eee-49a573caa19c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2505109532"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(article)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "8gdc4RPCHQv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 2"
      ],
      "metadata": {
        "id": "aXUv5d3M8d0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Открываем [сайт](http://parsinger.ru/html/index1_page_1.html)\n",
        "+ Проходимся по всем категориям, страницам и карточкам с товарами(всего 160шт)\n",
        "+ Собираем с каждой карточки стоимость товара умножая, на количество товара в наличии\n",
        "+ Складываем получившийся результат\n",
        "+ Получившуюся цифру с общей стоимостью всех товаров вставляем и выводим."
      ],
      "metadata": {
        "id": "xd707KgA8p8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://parsinger.ru/html/index1_page_1.html'\n",
        "father_url = 'https://parsinger.ru/html/'\n",
        "\n",
        "soup = get_response(url)\n",
        "\n",
        "list_of_expression = []\n",
        "count = 0\n",
        "for link in tqdm(soup.find(class_='nav_menu').find_all('a')):\n",
        "    level1_url = father_url + link['href']\n",
        "    soup1_level = get_response(level1_url)\n",
        "\n",
        "    for number in soup1_level.find(class_='pagen').find_all('a'):\n",
        "        level2_url = father_url + number['href']\n",
        "        soup2_level = get_response(level2_url)\n",
        "\n",
        "        for index in soup2_level.find(class_='item_card').find_all('a', class_='name_item'):\n",
        "            level3_url = father_url + index['href']\n",
        "            soup3_level = get_response(level3_url)\n",
        "            price = soup3_level.find('span', id='price').text\n",
        "            in_stock = soup3_level.find('span', id='in_stock').text\n",
        "            price_number, in_stock_number = (extract_numbers_from_string(price),\n",
        "                                             extract_numbers_from_string(in_stock))\n",
        "            price_number = int(price_number[0])\n",
        "            in_stock_number = int(in_stock_number[0])\n",
        "            if in_stock_number == 0:\n",
        "                list_of_expression.append(price_number * 1)\n",
        "            else:\n",
        "                list_of_expression.append(price_number * in_stock_number)\n",
        "\n",
        "print(f'\\n\\nСумма всех товаров на сайте: {sum(list_of_expression)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dsb4MfIM8gjQ",
        "outputId": "2d71d4b1-5f8e-49e6-a6d0-cdb0df99c965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [01:15<00:00, 15.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Сумма всех товаров на сайте: 45067195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "m6kfb3lGHPo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 3"
      ],
      "metadata": {
        "id": "EZcAh-8A-XDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ На  [сайте](https://parsinger.ru/table/1/index.html) расположена таблица\n",
        "+ Цель: собрать все уникальные числа из таблицы (кроме цифр в заголовке) и суммировать их;\n",
        "+ Полученный результат вывести на экран"
      ],
      "metadata": {
        "id": "C2uGl4Nn86zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://parsinger.ru/table/1/index.html'\n",
        "\n",
        "soup = get_response(url, 'html.parser')\n",
        "\n",
        "\n",
        "numberts = []\n",
        "\n",
        "for number in soup.find_all('td'):\n",
        "    num = float(number.text)\n",
        "    if num not in numberts:\n",
        "        numberts.append(num)\n",
        "(f'Сумма уникальных цифр в заголовке: {sum(numberts)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fCYXzsugAU0p",
        "outputId": "bea81663-057e-4e16-f1a3-0664a31dddcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Сумма уникальных цифр в заголовке: 1240.0959999999998'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "TozVwoVKHOZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 4"
      ],
      "metadata": {
        "id": "doJX_BYbAnhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ На  [сайте](https://parsinger.ru/table/2/index.html) расположена таблица;\n",
        "+ Цель: Собрать числа с 1го столбца и суммировать их;\n",
        "+ Полученный результат вывести на экран."
      ],
      "metadata": {
        "id": "3vi6k0qKAr1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://parsinger.ru/table/2/index.html'\n",
        "\n",
        "soup = get_response(url)\n",
        "\n",
        "first_col = [float(t.text) for t in soup.select('td:first-of-type')]\n",
        "\n",
        "print(f'Сумма чисел первого столбца: {sum(first_col)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_oXLRCbBAxI",
        "outputId": "f6756091-ac8c-4c77-8da9-b115b310f595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сумма чисел первого столбца: 80.726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "47ea4Uf8HM6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 5"
      ],
      "metadata": {
        "id": "miZ8QSwEBPPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ На  [сайте](https://parsinger.ru/table/3/index.html) расположена таблица;\n",
        "+ Цель: Собрать числа которые выделены жирным шрифтом и суммировать их;\n",
        "+ Полученный результат вывести на экран."
      ],
      "metadata": {
        "id": "9pFTeO9EBRQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://parsinger.ru/table/3/index.html'\n",
        "soup = get_response(url)\n",
        "\n",
        "all_bold_font = [float(b.text) for b in soup.find('table').find_all('b')]\n",
        "\n",
        "print(f'Сумма чисел выделенных жирным шрифтом: {sum(all_bold_font)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13737ljhDIdI",
        "outputId": "34cfb2e0-e0b4-4ec9-a414-46c742ff93b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сумма чисел выделенных жирным шрифтом: 373.32899999999995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "wcBUznN4HLmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 6\n",
        "\n"
      ],
      "metadata": {
        "id": "LiuD0w6ZCQAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ На  [сайте](https://parsinger.ru/table/4/index.html) расположена таблица;\n",
        "+ Цель: Собрать числа в зелёных ячейках и суммировать их;\n",
        "+ Полученный результат вывести на экран."
      ],
      "metadata": {
        "id": "GQrmNGnkCSi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://parsinger.ru/table/4/index.html'\n",
        "soup = get_response(url)\n",
        "\n",
        "all_green = [float(g.text) for g in soup.select('.green')]\n",
        "print(f'Сумма чисел в зеленых ячейках: {sum(all_green)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XgiU9wCDi8p",
        "outputId": "7bfc97be-78bb-408c-e0d3-5b39c4591086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сумма чисел в зеленых ячейках: 425.7659999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "qriKgtGXHKDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 7"
      ],
      "metadata": {
        "id": "vkVcjNE7DxmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ На  [сайте](https://parsinger.ru/table/5/index.html) расположена таблица;\n",
        "+ Цель: Умножить число в оранжевой ячейке на число в голубой ячейке в той же строке и всё суммировать;\n",
        "+ Полученный результат вывести на экран"
      ],
      "metadata": {
        "id": "Tx1FJIBwDz_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://parsinger.ru/table/5/index.html'\n",
        "soup = get_response(url)\n",
        "\n",
        "final_sum = []\n",
        "for tr in soup.select('tr:has(.orange)'):\n",
        "    orange = float(tr.select_one('.orange').text)\n",
        "    blue = float(tr.select_one('td:last-of-type').text)\n",
        "    final_sum.append(orange * blue)\n",
        "print(f'Сумма произведения чисел в оранжевой и голубой ячейках: {sum(final_sum)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9JMnuRtEAS1",
        "outputId": "804f0f7c-22e8-4942-e966-82afa14f5241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сумма произведения чисел в оранжевой и голубой ячейках: 2521465.6860000016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "PuH8UpDcHITR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 8"
      ],
      "metadata": {
        "id": "z-pajWAjEQ9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ На  [сайте](https://parsinger.ru/table/5/index.html) расположена таблица;\n",
        "+ Цель: Написать скрипт который формирует словарь, где ключ будет автоматически формироваться из заголовка столбцов, а значения это сумма всех чисел в столбце;\n",
        "+ Округлить каждое число до 3х символов после запятой.\n",
        "+ Полученный словарь вывести на экран.\n",
        "\n",
        "\n",
        "Пример ожидаемого словаря:\n",
        "\n",
        "{'1 column': 000.000, '2 column': 000.000, '3 column': 000.000, '4 column': 000.000, '5 column':\n",
        "000.00, '6 column': 000.000, '7 column': 000.000, '8 column': 000.000, '9 column': 000.000,\n",
        "'10 column': 000.000, '11 column': 000.000, '12 column': 000.000, '13 column': 000.000, '14 column':\n",
        "000.000, '15 column': 000000.0}"
      ],
      "metadata": {
        "id": "BwYqNfHwETVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://parsinger.ru/table/5/index.html'\n",
        "\n",
        "soup = get_response(url)\n",
        "\n",
        "final_tuple = {}\n",
        "\n",
        "for th in soup.select('th'):\n",
        "    for td in soup.select('tr:not(:has(th))'):\n",
        "        list_values = []\n",
        "        for value in td.text.strip().split('\\n'):\n",
        "            float_value = float(value)\n",
        "            list_values.append(round(float(value), 3))\n",
        "        final_tuple[th.text] = sum(list_values)\n",
        "\n",
        "\n",
        "display(final_tuple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "LSlCaYYyET8l",
        "outputId": "d1fa9c88-3f84-434d-d8af-41fae6468432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'1 column': 1346.644,\n",
              " '2 column': 1346.644,\n",
              " '3 column': 1346.644,\n",
              " '4 column': 1346.644,\n",
              " '5 column': 1346.644,\n",
              " '6 column': 1346.644,\n",
              " '7 column': 1346.644,\n",
              " '8 column': 1346.644,\n",
              " '9 column': 1346.644,\n",
              " '10 column': 1346.644,\n",
              " '11 column': 1346.644,\n",
              " '12 column': 1346.644,\n",
              " '13 column': 1346.644,\n",
              " '14 column': 1346.644,\n",
              " '15 column': 1346.644}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "R3uhDJ_dHGxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 9"
      ],
      "metadata": {
        "id": "ozDLuS_NEpsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишите код, который собирает данные в категории [HDD](https://parsinger.ru/html/index1_page_1.html) со всех 4х страниц и сохраняет всё в таблицу.\n",
        "\n",
        "![](https://ucarecdn.com/5c490d7b-bb04-45c3-a88b-a4bddd482e46/)\n",
        "\n",
        "\n",
        "\n",
        "Информация которую необходимо собрать.\n",
        "\n",
        "\n",
        "\n",
        "    Заголовки :  Наименование, Бренд, Форм-фактор, Ёмкость, Объём буф. памяти, Цена\n",
        "\n",
        "Пример заголовков\n",
        "\n",
        "![](https://ucarecdn.com/c7589858-d54c-463b-ab65-776eb6225a10/)\n"
      ],
      "metadata": {
        "id": "iZjBbKrIEtnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# начало ресерчинга\n",
        "url = 'https://parsinger.ru/html/index4_page_1.html'\n",
        "\n",
        "father_url = 'https://parsinger.ru/html/'\n",
        "\n",
        "# суп начала ресерчинга\n",
        "soup = get_response(url)\n",
        "\n",
        "# получаем признаки\n",
        "items = [item.text.strip() for item in soup.select('.description')[0]]\n",
        "col_names = ['Наименование'] + [item.split(':')[0].strip()\n",
        "            for item in items if item\n",
        "                    .split(':')[0]\n",
        "                    .strip() != ''] + ['Цена']\n",
        "# записываем признаки\n",
        "with open('hdd-list.csv', 'w', encoding='utf-8-sig',  newline='') as file:\n",
        "    writer = csv.writer(file, delimiter=';')\n",
        "    writer.writerow(col_names)\n",
        "\n",
        "# переменная файла для аппенда на итерациях\n",
        "file = open('hdd-list.csv', 'a', encoding='utf-8-sig', newline='')\n",
        "writer = csv.writer(file, delimiter=';')\n",
        "\n",
        "# линки по которым итерируемся\n",
        "iter_link =[link.get('href') for link in soup.select('div.pagen a')]\n",
        "\n",
        "# итерация по линкам\n",
        "for link in tqdm(iter_link):\n",
        "    # получаем линк страниц на каждой итерации\n",
        "    url = father_url + link\n",
        "    # получаем ингредиенты супа\n",
        "    soup = get_response(url)\n",
        "    # ждем\n",
        "    time.sleep(1)\n",
        "    # получаем наименование\n",
        "    name = [x.text.strip() for x in soup.select('a.name_item')]\n",
        "    # описание\n",
        "    description = [x.text.split('\\n') for x in soup.select('div.description')]\n",
        "    # цену\n",
        "    price = [x.text for x in soup.select('p.price')]\n",
        "\n",
        "    # итерируемся по значениям и собираем ингредиенты воедино\n",
        "    for item, descr, price in zip(name, description, price):\n",
        "        flatten = item, *[x.split(':')[1].strip()\n",
        "                        for x in descr if x], price\n",
        "        # записываем в файл\n",
        "        writer.writerow(flatten)\n",
        "\n",
        "# закрывем\n",
        "file.close()"
      ],
      "metadata": {
        "id": "8YN-8VWmFNY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "57RHQqaqHD5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 10"
      ],
      "metadata": {
        "id": "CAIArbRNFqpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишите код, который собирает данные со всех страниц и категорий на сайте [тренажере](http://parsinger.ru/html/index1_page_1.html) и сохраните всё в таблицу.\n",
        "\n",
        "Информация которую необходимо собрать.\n",
        "\n",
        "![](https://ucarecdn.com/5c490d7b-bb04-45c3-a88b-a4bddd482e46/)\n",
        "\n",
        " Заголовки :  Указывать не нужно\n",
        "\n",
        "Порядок колонок должен быть как на скрине ниже.\n",
        "\n",
        "![](https://ucarecdn.com/c1fec677-46f3-49c6-99fa-0dbb6c4617f0/)"
      ],
      "metadata": {
        "id": "CM42bpTUFtDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# начало ресерчинга\n",
        "url = 'https://parsinger.ru/html/index4_page_1.html'\n",
        "\n",
        "general_url = 'https://parsinger.ru/html/'\n",
        "# суп начала ресерчинга\n",
        "soup = get_response(url)\n",
        "\n",
        "# записываем признаки\n",
        "with open('product-list.csv', 'w', encoding='utf-8-sig',  newline='') as file:\n",
        "    writer = csv.writer(file, delimiter=';')\n",
        "# переменная файла для аппенда на итерациях\n",
        "file = open('product-list.csv', 'a', encoding='utf-8-sig', newline='')\n",
        "writer = csv.writer(file, delimiter=';')\n",
        "# линки категорий\n",
        "link_pages = [link.get('href') for link in soup.select('div.nav_menu a')]\n",
        "for page in tqdm(link_pages):\n",
        "# линки категорий по которым итерируемся\n",
        "    soup = get_response(general_url + page)\n",
        "    iter_link =[link.get('href') for link in soup.select('div.pagen a')]\n",
        "\n",
        "    # итерация по линкам страниц\n",
        "    for link in iter_link:\n",
        "        # получаем линк страниц на каждой итерации\n",
        "        url = father_url + link\n",
        "        # получаем ингредиенты супа\n",
        "        soup = get_response(url)\n",
        "        # спим\n",
        "        time.sleep(1)\n",
        "        # получаем наименование\n",
        "        name = [x.text.strip() for x in soup.select('a.name_item')]\n",
        "        # описание\n",
        "        description = [x.text.split('\\n') for x in soup.select('div.description')]\n",
        "        # цену\n",
        "        price = [x.text for x in soup.select('p.price')]\n",
        "\n",
        "        # итерируемся по значениям и собираем ингредиенты воедино\n",
        "        for item, descr, price in zip(name, description, price):\n",
        "            flatten = item, *[x.split(':')[1].strip()\n",
        "                            for x in descr if x], price\n",
        "            # записываем в файл\n",
        "            writer.writerow(flatten)\n",
        "\n",
        "# закрывем\n",
        "file.close()"
      ],
      "metadata": {
        "id": "3dkhwctSGT6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "oi02RdAVHB2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 11"
      ],
      "metadata": {
        "id": "RmOAfta8G7RN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишите код, который собирает данные в категории [watch](http://parsinger.ru/html/index1_page_1.html) c каждой карточки, всего их 32.\n",
        "\n",
        "**Информация которую необходимо собрать.**\n",
        "\n",
        "![](https://ucarecdn.com/f29cc133-f193-46dc-8dcc-b2096fd80678/)\n",
        "\n",
        "Обязательные Заголовки :  Наименование, Артикул, Бренд, Модель, Тип, Технология экрана, Материал корпуса, Материал браслета, Размер, Сайт производителя, Наличие, Цена, Старая цена,  Ссылка на карточку с товаром.\n",
        "\n",
        "Всего должно быть 14 заголовков\n",
        "\n",
        "Пример заголовков\n",
        "\n",
        "![](https://ucarecdn.com/97b2e668-238b-4e14-9ce0-55ff35633fbb/)\n",
        "\n",
        "p.s. Длинные заголовки можно сократить"
      ],
      "metadata": {
        "id": "4XObgrz3Hr9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# входной урл\n",
        "url = 'https://parsinger.ru/html/watch/1/1_2.html'\n",
        "\n",
        "# первый суп\n",
        "soup = get_response(url)\n",
        "\n",
        "# получаем столбцы\n",
        "art = [p.text.split(':')[0] for p in soup.select('.article')]\n",
        "col_desc = [de.strip().split(':')[0] for de in soup.find(id='description').text.strip().split('\\n')]\n",
        "last_cols = ['Наличие', 'Цена', 'Старая цена', 'Ссылка на карточку с товаром']\n",
        "\n",
        "# записываем первую строку признаков\n",
        "with open('watch-list.csv', 'w', encoding='utf-8', newline='') as f:\n",
        "    writer = csv.writer(f, delimiter=';')\n",
        "    writer.writerow(['Наименование']+ art + col_desc + last_cols)\n",
        "\n",
        "# начниаем ресерч\n",
        "url = 'https://parsinger.ru/html/index1_page_1.html'\n",
        "\n",
        "father_url = f\"{'/'.join(url.split('/')[:-1])}/\"\n",
        "# второй суп\n",
        "soup = get_response(url)\n",
        "\n",
        "# функция форматирования текста\n",
        "def form(tag):\n",
        "    tag = tag.text.split(':')[-1].strip()\n",
        "    return tag\n",
        "\n",
        "# переменная пагинации\n",
        "pagen = [page.get('href') for page in soup.select_one('div.pagen').find_all('a')]\n",
        "\n",
        "# списки объектов\n",
        "named = []\n",
        "articl = []\n",
        "descriptions = []\n",
        "in_stocks = []\n",
        "prices = []\n",
        "old_price = []\n",
        "product_url = []\n",
        "\n",
        "# пагинация\n",
        "for page in tqdm(pagen):\n",
        "    # получаем первый урл и суп\n",
        "    url = urljoin(father_url, page)\n",
        "    soup = get_response(url)\n",
        "\n",
        "    # пошли в карточки\n",
        "    product_card = [prod.get('href') for prod in soup.select('.sale_button a')]\n",
        "    for product in product_card:\n",
        "        # второй уровень - урл, суп\n",
        "        url = urljoin(father_url, product)\n",
        "        soup = get_response(url)\n",
        "\n",
        "        # получаем все необходимые данные\n",
        "        names = [name.text.strip() for name in soup.select('#p_header')]\n",
        "        article = [form(art) for art in soup.select('.article')]\n",
        "        description = [form(des) for des in soup.find(id='description').find_all('li')]\n",
        "        in_stock = [form(stock) for stock in soup.select('#in_stock')]\n",
        "        price = [old_p.text for old_p in soup.select('span[id$=\"price\"]')]\n",
        "\n",
        "        # добавляем в списки объекты\n",
        "        named.append(names)\n",
        "        articl.append(article)\n",
        "        descriptions.append(description)\n",
        "        in_stocks.append(in_stock)\n",
        "        prices.append(price)\n",
        "        product_url.append(url)\n",
        "\n",
        "# итерируемся и записываем\n",
        "for name, art, descr, stock, price, link in zip(named, articl,\n",
        "                                                descriptions, in_stocks,\n",
        "                                                prices, product_url):\n",
        "    row = *name, *art, *descr, *stock, *price, link\n",
        "    file = open('watch-list.csv', 'a', encoding='utf-8-sig', newline='')\n",
        "    writer = csv.writer(file, delimiter=';')\n",
        "    writer.writerow(row)\n",
        "\n",
        "# закрываем и выводим ответ\n",
        "file.close()\n",
        "print('Файл создан')"
      ],
      "metadata": {
        "id": "TXtGSQLPIFFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "MTCbZPkjIbcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 12"
      ],
      "metadata": {
        "id": "wY7-YFTAIdWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишите код, который собирает данные в каждой категории c каждой карточки, всего их 160.\n",
        "\n",
        "Информация которую необходимо собрать.\n",
        "\n",
        "![](https://ucarecdn.com/49455ee6-6810-47af-a4d8-b8f6485b4d70/)\n",
        "\n",
        "Обязательные Заголовки :  Наименование, Артикул, Бренд, Модель, Наличие, Цена, Старая цена, Ссылка на карточку с товаром,\n",
        "\n",
        "Перечисленные заголовки являются общими для всех карточек.\n",
        "\n",
        "Пример заголовков.\n",
        "\n",
        " ![](https://ucarecdn.com/04a8d7d1-60cb-47c0-834e-7ce3016b91fa/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oF95xk8eIccL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# входной урл\n",
        "url = 'https://parsinger.ru/html/watch/1/1_2.html'\n",
        "\n",
        "# первый суп\n",
        "soup = get_response(url)\n",
        "\n",
        "# получаем столбцы\n",
        "art = [p.text.split(':')[0] for p in soup.select('.article')]\n",
        "col_desc = [de.strip().split(':')[0] for de in soup.find(id='description').text.strip().split('\\n')]\n",
        "last_cols = ['Наличие', 'Цена', 'Старая цена', 'Ссылка на карточку с товаром']\n",
        "\n",
        "# записываем первую строку признаков\n",
        "with open('prod-list.csv', 'w', encoding='utf-8', newline='') as f:\n",
        "    writer = csv.writer(f, delimiter=';')\n",
        "    writer.writerow(['Наименование']+ art + col_desc[:2] + last_cols)\n",
        "\n",
        "# начниаем ресерч\n",
        "url = 'https://parsinger.ru/html/index1_page_1.html'\n",
        "\n",
        "father_url = f\"{'/'.join(url.split('/')[:-1])}/\"\n",
        "# второй суп\n",
        "soup = get_response(url)\n",
        "\n",
        "# функция форматирования текста\n",
        "def form(tag):\n",
        "    tag = tag.text.split(':')[-1].strip()\n",
        "    return tag\n",
        "\n",
        "# списки объектов\n",
        "named = []\n",
        "articl = []\n",
        "descriptions = []\n",
        "in_stocks = []\n",
        "prices = []\n",
        "old_price = []\n",
        "product_url = []\n",
        "\n",
        "# переменная каталога\n",
        "catalog_pages = [a['href'] for a in soup.find('div', 'nav_menu').find_all('a')]\n",
        "\n",
        "# пагинация по каталогу\n",
        "for cat in tqdm(catalog_pages):\n",
        "    url = urljoin(father_url, cat)\n",
        "    soup = get_response(url)\n",
        "\n",
        "    # переменная пагинации\n",
        "    pagen = [page.get('href') for page in soup.select_one('div.pagen').find_all('a')]\n",
        "    for page in pagen:\n",
        "        # получаем второй урл и суп\n",
        "        url = urljoin(father_url, page)\n",
        "        soup = get_response(url)\n",
        "\n",
        "        # пошли в карточки\n",
        "        product_card = [prod.get('href') for prod in soup.select('.sale_button a')]\n",
        "        for product in product_card:\n",
        "            # третий уровень - урл, суп\n",
        "            url = urljoin(father_url, product)\n",
        "            soup = get_response(url)\n",
        "\n",
        "            # получаем все необходимые данные\n",
        "            names = [name.text.strip() for name in soup.select('#p_header')]\n",
        "            article = [form(art) for art in soup.select('.article')]\n",
        "            description = [form(des) for des in soup.find(id='description').find_all('li')]\n",
        "            in_stock = [form(stock) for stock in soup.select('#in_stock')]\n",
        "            price = [old_p.text for old_p in soup.select('span[id$=\"price\"]')]\n",
        "\n",
        "            # добавляем в списки объекты\n",
        "            named.append(names)\n",
        "            articl.append(article)\n",
        "            descriptions.append(description[:2])\n",
        "            in_stocks.append(in_stock)\n",
        "            prices.append(price)\n",
        "            product_url.append(url)\n",
        "\n",
        "# итерируемся и записываем\n",
        "for name, art, descr, stock, price, link in zip(named, articl,\n",
        "                                                descriptions, in_stocks,\n",
        "                                                prices, product_url):\n",
        "    row = *name, *art, *descr, *stock, *price, link\n",
        "    file = open('prod-list.csv', 'a', encoding='utf-8-sig', newline='')\n",
        "    writer = csv.writer(file, delimiter=';')\n",
        "    writer.writerow(row)\n",
        "\n",
        "# закрываем и выводим ответ\n",
        "file.close()\n",
        "print('Файл создан')"
      ],
      "metadata": {
        "id": "wy2M-bXdIrJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "-HL4fLHaiBMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 13"
      ],
      "metadata": {
        "id": "XLjP9_9khhxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выберите 1 любую категорию на сайте [тренажере](http://parsinger.ru/html/index1_page_1.html) и соберите все данные с карточек.\n",
        "\n",
        "![](https://ucarecdn.com/403eeeac-1048-4bdf-9689-d5b5016bb40c/)\n",
        "\n",
        "По результату выполнения кода должен появится файл .json с отступом в 4 пробела\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6OGaMuCihkau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://parsinger.ru/html/index2_page_1.html'\n",
        "\n",
        "soup = get_response(url)\n",
        "\n",
        "keys = (\n",
        "    ['Наименование'] + [li.text.split(':')[0] for li in soup\n",
        "                                .select('.description')[0]\n",
        "                                .find_all('li')] + ['Цена'])\n",
        "\n",
        "res = []\n",
        "names = [n.text for n in soup.select('.name_item')]\n",
        "description = [x.text.strip().split('\\n') for x in soup.select('div.description')]\n",
        "prices = [d.text for d in soup.select('p.price')]\n",
        "\n",
        "for name, desc, price in zip(names, description, prices):\n",
        "    res.append({\n",
        "        keys[0]: name,\n",
        "        keys[1]: desc[0].split(':')[1],\n",
        "        keys[2]: desc[1].split(':')[1],\n",
        "        keys[3]: desc[2].split(':')[1],\n",
        "        keys[4]: desc[3].split(':')[1],\n",
        "        keys[5]: price\n",
        "    })\n",
        "\n",
        "with open('res.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(res, file, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "IV2Bi7-yh5rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "AuF7cyUnh_mA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 14"
      ],
      "metadata": {
        "id": "9siQEFlhiCvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Соберите данные со всех 5 категорий на сайте [тренажере](http://parsinger.ru/html/index1_page_1.html) и соберите все данные с карточек.\n",
        "\n",
        "\n",
        "![](https://ucarecdn.com/f958fc45-0139-4c56-9539-189d1615f6cb/)\n",
        "\n",
        "По результату выполнения кода должен появится файл .json с отступом в 4 пробела\n",
        "\n",
        "Пример:\n",
        "\n",
        "![](https://ucarecdn.com/9088b7b7-7041-41ff-94c2-ce6dbf7ecee3/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RfsuF3DXiE8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# стартовый урл\n",
        "url = 'https://parsinger.ru/html/index1_page_1.html'\n",
        "\n",
        "res1 = []\n",
        "\n",
        "soup = get_response(url)\n",
        "father_url = f\"{'/'.join(url.split('/')[:-1])}/\"\n",
        "# переменная каталога\n",
        "catalog_pages = [a['href'] for a in soup.find('div', 'nav_menu').find_all('a')]\n",
        "\n",
        "# первый уровень\n",
        "for cat in tqdm(catalog_pages):\n",
        "    url = urljoin(father_url, cat)\n",
        "    soup = get_response(url)\n",
        "      # переменная пагинации\n",
        "    pagen = [page.get('href') for page in soup.select_one('div.pagen').find_all('a')]\n",
        "\n",
        "    # второй уровень\n",
        "    for page in pagen:\n",
        "        # получаем второй урл и суп\n",
        "        url = urljoin(father_url, page)\n",
        "        soup = get_response(url)\n",
        "\n",
        "        keys = (\n",
        "            ['Наименование'] + [li.text.split(': ')[0] for li in soup\n",
        "                                        .select('.description')[0]\n",
        "                                        .find_all('li')] + ['Цена'])\n",
        "\n",
        "\n",
        "        names = [n.text for n in soup.select('.name_item')]\n",
        "        description = [x.text.strip().split('\\n') for x in soup.select('div.description')]\n",
        "        prices = [d.text for d in soup.select('p.price')]\n",
        "\n",
        "        for name, desc, price in zip(names, description, prices):\n",
        "            res1.append({\n",
        "                keys[0]: name,\n",
        "                keys[1]: desc[0].split(':')[1].strip(),\n",
        "                keys[2]: desc[1].split(':')[1].strip(),\n",
        "                keys[3]: desc[2].split(':')[1].strip(),\n",
        "                keys[4]: desc[3].split(':')[1].strip(),\n",
        "                keys[5]: price\n",
        "            })\n",
        "\n",
        "with open('res1.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(res1, file, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "qkGiJP40iYRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "e-ovDL9mipVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 15"
      ],
      "metadata": {
        "id": "wa3rI5wGiqy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выберите 1 любую категорию на сайте [тренажёре](http://parsinger.ru/html/index3_page_1.html), и соберите все данные с карточек товаров + ссылка на карточку.\n",
        "\n",
        "![](https://ucarecdn.com/f84ae3da-5341-4295-ab9b-13c35599234c/)\n",
        "\n",
        "По результату выполнения кода должен появится файл .json с отступом в 4 пробела. Ключи в блоке description должны быть получены автоматически из атрибутов HTML элементов.\n",
        "\n",
        "\n",
        "\n",
        "Пример:\n",
        "\n",
        "![](https://ucarecdn.com/5f6db4be-c93c-4851-a99d-f1c31e49cfbf/)"
      ],
      "metadata": {
        "id": "FchiXAh8iqcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# стартовый урл\n",
        "url = 'https://parsinger.ru/html/watch/1/1_2.html'\n",
        "\n",
        "# первый суп\n",
        "soup = get_response(url)\n",
        "\n",
        "# получаем столбцы\n",
        "art = [p.text.split(':')[0] for p in soup.select('.article')]\n",
        "col_desc = [de.strip().split(':')[0] for de in soup.find(id='description').text.strip().split('\\n')]\n",
        "last_cols = ['Наличие', 'Цена', 'Старая цена', 'Ссылка на карточку с товаром']\n",
        "\n",
        "# записываем первую строку признаков\n",
        "with open('prod-list.csv', 'w', encoding='utf-8', newline='') as f:\n",
        "    writer = csv.writer(f, delimiter=';')\n",
        "    writer.writerow(['Наименование']+ art + col_desc[:2] + last_cols)\n",
        "\n",
        "# начниаем ресерч\n",
        "url = 'https://parsinger.ru/html/index1_page_1.html'\n",
        "\n",
        "father_url = f\"{'/'.join(url.split('/')[:-1])}/\"\n",
        "# второй суп\n",
        "soup = get_response(url)\n",
        "\n",
        "# функция форматирования текста\n",
        "def form(tag):\n",
        "    tag = tag.text.split(':')[-1].strip()\n",
        "    return tag\n",
        "\n",
        "# списки объектов\n",
        "result = []\n",
        "\n",
        "\n",
        "# переменная пагинации\n",
        "pagen = [page.get('href') for page in soup.select_one('div.pagen').find_all('a')]\n",
        "for page in tqdm(pagen):\n",
        "    # получаем второй урл и суп\n",
        "    url = urljoin(father_url, page)\n",
        "    soup = get_response(url)\n",
        "\n",
        "    # переходим в карточки\n",
        "    product_card = [prod.get('href') for prod in soup.select('.sale_button a')]\n",
        "    for product in product_card:\n",
        "        # третий уровень - урл, суп\n",
        "        url = urljoin(father_url, product)\n",
        "        soup = get_response(url)\n",
        "\n",
        "        art = [p.text.split(':')[0] for p in soup.select('.article')]\n",
        "        col_desc = [de.strip().split(':')[0] for de in soup.find(id='description').text.strip().split('\\n')]\n",
        "        last_cols = ['Наличие', 'Цена', 'Старая цена', 'Ссылка на карточку с товаром']\n",
        "\n",
        "        keys = (['Наименование']+ art + col_desc + last_cols)\n",
        "\n",
        "        # получаем все необходимые данные\n",
        "        names = [name.text.strip() for name in soup.select('#p_header')]\n",
        "        article = [form(art) for art in soup.select('.article')]\n",
        "        description = [form(des) for des in soup.find(id='description').find_all('li')]\n",
        "        in_stock = [form(stock) for stock in soup.select('#in_stock')]\n",
        "        price = [old_p.text for old_p in soup.select('span[id$=\"price\"]')]\n",
        "\n",
        "        # добавляем объекты\n",
        "        for name, art, stock in zip(names, article, in_stock):\n",
        "            result.append({\n",
        "                keys[0]: name,\n",
        "                keys[1]: art,\n",
        "                keys[2]: description[0],\n",
        "                keys[3]: description[1],\n",
        "                keys[4]: description[2],\n",
        "                keys[5]: description[3],\n",
        "                keys[6]: description[4],\n",
        "                keys[7]: description[5],\n",
        "                keys[8]: description[6],\n",
        "                keys[9]: description[7],\n",
        "                keys[10]: stock,\n",
        "                keys[11]: prices[0],\n",
        "                keys[12]: prices[1],\n",
        "                keys[13]: url,\n",
        "            })\n",
        "\n",
        "with open('result1.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(result, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print('Файл создан')"
      ],
      "metadata": {
        "id": "qxdmJiymjBQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "kopLIPVgjaD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 16"
      ],
      "metadata": {
        "id": "8Id7I_41jb91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Соберите данные со всех 5 категорий на сайте [тренажере](http://parsinger.ru/html/index1_page_1.html) и соберите все данные с карточек + ссылка на карточку с товаром.\n",
        "\n",
        "![](https://ucarecdn.com/5fe217bc-6960-4220-8286-caa2c910ca84/)\n",
        "\n",
        "По результату выполнения кода должен появится файл .json с отступом в 4 пробела. Ключи в блоке description должны быть получены автоматически из атрибутов HTML элементов.\n",
        "\n",
        "\n",
        "\n",
        "Пример:\n",
        "\n",
        "![](https://ucarecdn.com/0c1bb5e4-0273-4c9a-8335-c0476497a23d/)"
      ],
      "metadata": {
        "id": "1Bcib1BMjlud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# стартовый урл\n",
        "url = 'https://parsinger.ru/html/watch/1/1_2.html'\n",
        "\n",
        "# первый суп\n",
        "soup = get_response(url)\n",
        "\n",
        "# получаем столбцы\n",
        "art = [p.text.split(':')[0] for p in soup.select('.article')]\n",
        "col_desc = [de.strip().split(':')[0] for de in soup.find(id='description').text.strip().split('\\n')]\n",
        "last_cols = ['Наличие', 'Цена', 'Старая цена', 'Ссылка на карточку с товаром']\n",
        "\n",
        "# записываем первую строку признаков\n",
        "with open('prod-list.csv', 'w', encoding='utf-8', newline='') as f:\n",
        "    writer = csv.writer(f, delimiter=';')\n",
        "    writer.writerow(['Наименование']+ art + col_desc[:2] + last_cols)\n",
        "\n",
        "\n",
        "url = 'https://parsinger.ru/html/index1_page_1.html'\n",
        "\n",
        "father_url = f\"{'/'.join(url.split('/')[:-1])}/\"\n",
        "# второй суп\n",
        "soup = get_response(url)\n",
        "\n",
        "# функция форматирования текста\n",
        "def form(tag):\n",
        "    tag = tag.text.split(':')[-1].strip()\n",
        "    return tag\n",
        "\n",
        "# списки объектов\n",
        "result = []\n",
        "\n",
        "# переменная каталога\n",
        "catalog_pages = [a['href'] for a in soup.find('div', 'nav_menu').find_all('a')]\n",
        "\n",
        "for cat in tqdm(catalog_pages):\n",
        "    url = urljoin(father_url, cat)\n",
        "    soup = get_response(url)\n",
        "\n",
        "    # переменная пагинации\n",
        "    pagen = [page.get('href') for page in soup.select_one('div.pagen').find_all('a')]\n",
        "    for page in pagen:\n",
        "        # получаем второй урл и суп\n",
        "        url = urljoin(father_url, page)\n",
        "        soup = get_response(url)\n",
        "\n",
        "        # пошли в карточки\n",
        "        product_card = [prod.get('href') for prod in soup.select('.sale_button a')]\n",
        "        for product in product_card:\n",
        "            # третий уровень - урл, суп\n",
        "            url = urljoin(father_url, product)\n",
        "            soup = get_response(url)\n",
        "\n",
        "            art = [p.text.split(':')[0] for p in soup.select('.article')]\n",
        "            col_desc = [de.strip().split(':')[0] for de in    soup.find(id='description').text.strip().split('\\n')]\n",
        "            last_cols = ['Наличие', 'Цена', 'Старая цена', 'Ссылка на карточку с товаром']\n",
        "\n",
        "            keys = (['Наименование']+ art + col_desc + last_cols)\n",
        "\n",
        "            # получаем все необходимые данные\n",
        "            names = [name.text.strip() for name in soup.select('#p_header')]\n",
        "            article = [form(art) for art in soup.select('.article')]\n",
        "            description = [form(des) for des in soup.find(id='description').find_all('li')]\n",
        "            in_stock = [form(stock) for stock in soup.select('#in_stock')]\n",
        "            price = [old_p.text for old_p in soup.select('span[id$=\"price\"]')]\n",
        "\n",
        "            # добавляем объекты\n",
        "            for name, art, stock in zip(names, article, in_stock):\n",
        "                result.append({\n",
        "                    keys[0]: name,\n",
        "                    keys[1]: art,\n",
        "                    keys[2]: description[0],\n",
        "                    keys[3]: description[1],\n",
        "                    keys[4]: description[2],\n",
        "                    keys[5]: description[3],\n",
        "                    keys[6]: description[4],\n",
        "                    keys[7]: description[5],\n",
        "                    keys[8]: description[6],\n",
        "                    keys[9]: description[7],\n",
        "                    keys[10]: stock,\n",
        "                    keys[11]: prices[0],\n",
        "                    keys[12]: prices[1],\n",
        "                    keys[13]: url,\n",
        "                })\n",
        "\n",
        "with open('result.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(result, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print('Файл создан')"
      ],
      "metadata": {
        "id": "QW0PCCCwj4au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "DeT79VkukV3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задача 17"
      ],
      "metadata": {
        "id": "vg4ACoX9kSmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используйте полученный по [ссылке](http://parsinger.ru/downloads/get_json/res.json) JSON, чтобы посчитать количество товара в каждой категории.\n",
        "\n",
        "На вход ожидается словарь {'watch': N, 'mobile': N, 'mouse': N, 'hdd': N, 'headphones': N}, где N - это общее количество товаров\n",
        "\n",
        "Количество вы найдёте в каждой карточке товара."
      ],
      "metadata": {
        "id": "0wO1VjlzkW_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url ='https://parsinger.ru/downloads/get_json/res.json'\n",
        "\n",
        "response = requests.get(url=url).json()\n",
        "\n",
        "keys = list(set([item['categories'] for item in response]))[::-1]\n",
        "\n",
        "category_counts = {key: 0 for key in keys}\n",
        "\n",
        "for item in response:\n",
        "    category = item['categories']\n",
        "    if category in category_counts:\n",
        "        category_counts[category] += int(item['count'])\n",
        "\n",
        "print(category_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9pxXZfPlP-g",
        "outputId": "33c2dc75-f508-40c2-99fb-b29edf6f3786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'mouse': 2692, 'mobile': 820, 'watch': 853, 'headphones': 1006, 'hdd': 1273}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "isFb95r8mhT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Практика парсинга курса криптовалюты"
      ],
      "metadata": {
        "id": "4IQyiPdM--PZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запрашиваем курс криптовалюты на ajax"
      ],
      "metadata": {
        "id": "VN-mMAgG_OyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Тестирование запросов перед созданием универсального интерфейса\n",
        "headers = {\n",
        "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36',\n",
        "    'x-requested-with': 'XMLHttpRequest'\n",
        "}\n",
        "\n",
        "url = \"https://bitality.cc/Home/GetSum?GiveName=Tinkoff&GetName=Bitcoin&Sum=1&Direction=1\"\n",
        "response = requests.get(url=url, headers=headers).json()\n",
        "print(response)"
      ],
      "metadata": {
        "id": "1pe6wHv6_gv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Тестирование с интерфейсом в лице функции\n",
        "url = 'https://bitality.cc/Home/GetSum' # с вопросом в конце и без работает, реквест подставляет сам вопрос\n",
        "\n",
        "data = param_search('Sberbank', 'Bitcoin', 5_000)\n",
        "\n",
        "headers = get_headers(1)\n",
        "\n",
        "response = requests.get(url=url,headers=headers ,params=data).json()\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WC2aLT__CQr",
        "outputId": "0fcfe89a-7610-443b-ea4c-0cad90d89738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'giveSum': 5000.0, 'getSum': '0.00194801'}\n"
          ]
        }
      ]
    }
  ]
}